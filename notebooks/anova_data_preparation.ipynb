{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "WORKSPACE_PATH = os.getenv(\"WORKSPACE_PATH\")\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(WORKSPACE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataframe_utils import read_excel_file, write_to_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = Path(os.getenv(\"RESULTS_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_column_parts(df: pd.DataFrame, patterns: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Splits column names into parts based on specified regex patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame whose column names need to be split.\n",
    "    - patterns (dict): A dictionary with regex patterns for each part to extract.\n",
    "                       Keys represent the part name, values contain regex strings.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with columns representing extracted parts from the\n",
    "                    original column names in `df`.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to store the separated parts of column names\n",
    "    parts_df = pd.DataFrame(index=df.columns)\n",
    "\n",
    "    # Apply each regex pattern to extract parts of column names\n",
    "    for part, pattern in patterns.items():\n",
    "        # Find all matches for the regex pattern\n",
    "        parts = df.columns.str.findall(pattern)\n",
    "\n",
    "        # Extract the first match for each column or set as None if no match is found\n",
    "        parts_df[part] = [x[0] if len(x) > 0 else None for x in parts]\n",
    "\n",
    "    return parts_df\n",
    "\n",
    "\n",
    "def sort_columns_by_rules(\n",
    "    df: pd.DataFrame, patterns: dict, sorting_rules: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sorts columns in the DataFrame based on extracted parts from column names and sorting rules.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with columns to be sorted.\n",
    "    - patterns (dict): A dictionary where keys are part names and values are regex patterns\n",
    "                       to extract those parts from the column names.\n",
    "    - sorting_rules (dict): A dictionary specifying the order for each extracted part.\n",
    "                            Keys represent part names, and values are lists with the\n",
    "                            desired sort order for that part.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with columns sorted according to the specified rules in `sorting_rules`.\n",
    "    \"\"\"\n",
    "    # Extract parts of column names using specified regex patterns\n",
    "    parts_df = extract_column_parts(df, patterns)\n",
    "    parts_df[\"original_column\"] = (\n",
    "        df.columns\n",
    "    )  # Store original column names for reordering\n",
    "\n",
    "    # Apply sorting rules for each part based on provided order lists\n",
    "    for col_name, order in sorting_rules.items():\n",
    "        parts_df[col_name] = pd.Categorical(\n",
    "            parts_df[col_name], categories=order, ordered=True\n",
    "        )\n",
    "\n",
    "    # Sort parts_df based on sorting rules to get the correct column order\n",
    "    sorted_parts_df = parts_df.sort_values(list(sorting_rules.keys()))\n",
    "\n",
    "    # Reorder columns in the original DataFrame based on sorted column names\n",
    "    return df[sorted_parts_df[\"original_column\"].values]\n",
    "\n",
    "\n",
    "def process_measurement_data(df: pd.DataFrame, value: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the input DataFrame to extract measurement types, pairs, and\n",
    "    create a pivot table with combined column names.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing measurement data with necessary columns.\n",
    "    - measure (str): The type of measure to be used in the DataFrame.\n",
    "    - value (str): The value to use for the pivot table values (e.g., 'corr' or 'shift_diff').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with pivoted columns and combined names.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If required columns are not present in the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if necessary columns are present in the DataFrame\n",
    "    required_columns = [\n",
    "        \"name_meas1\",\n",
    "        \"name_meas2\",\n",
    "        \"meas_number\",\n",
    "        \"condition\",\n",
    "        \"pair_number\",\n",
    "        \"task\",\n",
    "        value,\n",
    "    ]\n",
    "    for column in required_columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {column}\")\n",
    "\n",
    "    df = copy.deepcopy(df)\n",
    "\n",
    "    # Convert 'pair' to numeric type\n",
    "    df[\"pair_number\"] = df[\"pair_number\"].astype(int)\n",
    "\n",
    "    # Create a pivot table based on the specified value\n",
    "    result = df.pivot_table(\n",
    "        index=\"pair_number\",\n",
    "        columns=[\"meas_number\", \"condition\", \"task\"],\n",
    "        values=value,  # Use 'value' as specified to pull correct column data\n",
    "        aggfunc=\"first\",\n",
    "    )  # Use 'first' or another aggregation function as needed\n",
    "\n",
    "    # Reset index to convert the pivot table back to a DataFrame\n",
    "    result = result.reset_index()\n",
    "\n",
    "    # Combine the columns into a single name using underscore as a separator\n",
    "    result.columns = [\"pair_number\"] + [\n",
    "        \"_\".join(map(str, col)) for col in result.columns[1:]\n",
    "    ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable to choose between spearman results and pearson results\n",
    "corr_name = \"spearman\"  # Options: \"spearman\", \"person\"\n",
    "# Define a variable to choose between best results and all results\n",
    "result_choice = \"best\"  # Options: \"best\", \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_patterns = {\n",
    "    \"meas_number\": r\"^(1|2)\",\n",
    "    \"condition\": r\"_(C|R)_\",\n",
    "    \"task\": r\"_(baseline1|baseline2|z\\d(?:_\\d_[fm])?)_\",\n",
    "    \"value_name\": r\"_(CORR|SHIFT)_\",\n",
    "    \"measure_type\": r\"_(HR|SDNN|RMSSD)$\",\n",
    "}\n",
    "\n",
    "task_order = [\n",
    "    \"baseline1\",\n",
    "    \"z\",\n",
    "    \"z1\",\n",
    "    \"z1_1_f\",\n",
    "    \"z1_2_m\",\n",
    "    \"z1_3_f\",\n",
    "    \"z1_4_m\",\n",
    "    \"z1_5_f\",\n",
    "    \"z1_6_m\",\n",
    "    \"z2\",\n",
    "    \"z2_1_m\",\n",
    "    \"z2_2_f\",\n",
    "    \"z2_3_m\",\n",
    "    \"z2_4_f\",\n",
    "    \"z2_5_m\",\n",
    "    \"z2_6_f\",\n",
    "    \"baseline2\",\n",
    "]\n",
    "\n",
    "sorting_rules = {\"meas_number\": [\"1\", \"2\"], \"condition\": [\"R\", \"C\"], \"task\": task_order}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process HR results\n",
    "hr_results = read_excel_file(\n",
    "    RESULTS_DIR / \"analysis_data\" / corr_name / str(result_choice + \"_hr_results.xlsx\")\n",
    ")\n",
    "hr_corr_processed_data = process_measurement_data(hr_results, \"corr\")\n",
    "hr_shift_processed_data = process_measurement_data(hr_results, \"shift_diff\")\n",
    "hr_anova_data = hr_corr_processed_data.merge(\n",
    "    hr_shift_processed_data, on=\"pair_number\", suffixes=(\"_CORR\", \"_SHIFT\")\n",
    ")\n",
    "hr_anova_data.set_index(\"pair_number\", inplace=True)\n",
    "hr_anova_data = hr_anova_data.add_suffix(\"_HR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process SD results\n",
    "sdnn_results = read_excel_file(\n",
    "    RESULTS_DIR\n",
    "    / \"analysis_data\"\n",
    "    / corr_name\n",
    "    / str(result_choice + \"_sdnn_results.xlsx\")\n",
    ")\n",
    "sdnn_corr_processed_data = process_measurement_data(sdnn_results, \"corr\")\n",
    "sdnn_shift_processed_data = process_measurement_data(sdnn_results, \"shift_diff\")\n",
    "sdnn_anova_data = sdnn_corr_processed_data.merge(\n",
    "    sdnn_shift_processed_data, on=\"pair_number\", suffixes=(\"_CORR\", \"_SHIFT\")\n",
    ")\n",
    "sdnn_anova_data.set_index(\"pair_number\", inplace=True)\n",
    "sdnn_anova_data = sdnn_anova_data.add_suffix(\"_SDNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process RMSSD results\n",
    "rmssd_results = read_excel_file(\n",
    "    RESULTS_DIR\n",
    "    / \"analysis_data\"\n",
    "    / corr_name\n",
    "    / str(result_choice + \"_rmssd_results.xlsx\")\n",
    ")\n",
    "rmssd_corr_processed_data = process_measurement_data(rmssd_results, \"corr\")\n",
    "rmssd_shift_processed_data = process_measurement_data(rmssd_results, \"shift_diff\")\n",
    "rmssd_anova_data = rmssd_corr_processed_data.merge(\n",
    "    rmssd_shift_processed_data, on=\"pair_number\", suffixes=(\"_CORR\", \"_SHIFT\")\n",
    ")\n",
    "rmssd_anova_data.set_index(\"pair_number\", inplace=True)\n",
    "rmssd_anova_data = rmssd_anova_data.add_suffix(\"_RMSSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all anova dataframes on index\n",
    "anova_data = hr_anova_data.merge(\n",
    "    sdnn_anova_data, left_index=True, right_index=True, suffixes=(\"_HR\", \"_SDNN\")\n",
    ").merge(rmssd_anova_data, left_index=True, right_index=True, suffixes=(\"\", \"_RMSSD\"))\n",
    "\n",
    "# Dropping columns with \"R\" and \"SHIFT\" in name\n",
    "columns_to_drop = [col for col in anova_data.columns if \"R\" in col and \"SHIFT\" in col]\n",
    "anova_data = anova_data.drop(columns=columns_to_drop)\n",
    "anova_data = sort_columns_by_rules(anova_data, regex_patterns, sorting_rules)\n",
    "anova_data.reset_index(names=\"pair_number\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_excel(\n",
    "    anova_data,\n",
    "    ANALYSIS_DATA_DIR\n",
    "    / \"ANOVA\"\n",
    "    / str(corr_name + \"_\" + result_choice + \"_anova_data.xlsx\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.comments import Comment\n",
    "\n",
    "# Load workbook and select the active sheet\n",
    "wb = openpyxl.load_workbook(\n",
    "    ANALYSIS_DATA_DIR\n",
    "    / \"ANOVA\"\n",
    "    / str(corr_name + \"_\" + result_choice + \"_anova_data.xlsx\")\n",
    ")\n",
    "ws = wb.active\n",
    "\n",
    "note = (\n",
    "    \"Variable Naming Convention:\\n\"\n",
    "    \"Each variable name in this dataset follows a structured format to specify its measurement details.\\n\"\n",
    "    \"<meas_number>_<condition>_<task>_<value_name>_<measure_type>\\n\\n\"\n",
    "    \"Example: '1_C_z1_1_f_CORR_HR'\\n\\n\"\n",
    "    \"- meas_number (e.g., 1): Measurement number:\\n\"\n",
    "    \"    - '1' for first session before intervention\\n\"\n",
    "    \"    - '2' for second session after intervention\\n\"\n",
    "    \"- condition (e.g., C): Condition:\\n\"\n",
    "    \"    - 'C' for Cooperation\\n\"\n",
    "    \"    - 'R' for Relaxation\\n\"\n",
    "    \"- task (e.g., z1_1_f): The specific task performed by the participants.\\n\"\n",
    "    \"    - for condition 'C':\\n\"\n",
    "    \"       - z<exercise_number>_<exercise_round>_<gender>\\n\"\n",
    "    \"           - exercise_number:\\n\"\n",
    "    \"               - '1' for first exercise in condition\\n\"\n",
    "    \"               - '2' for second exercise in condition\\n\"\n",
    "    \"           - exercise_round:\\n\"\n",
    "    \"               - '1' to '6': indicates the sequential round of the exercise within the task (e.g., '1' for the first round, '2' for the second, etc.)\\n\"\n",
    "    \"           - gender:\\n\"\n",
    "    \"               - 'f' for female and 'm' for male: indicates who was instructed to lead in the exercise\\n\"\n",
    "    \"       - z<exercise_number>: all exercises parts in specific exercise\\n\"\n",
    "    \"           - exercise_number:\\n\"\n",
    "    \"               - '1' for first exercise in condition\\n\"\n",
    "    \"               - '2' for second exercise in condition\\n\"\n",
    "    \"    - for condition 'R':\\n\"\n",
    "    \"       - z\\n\"\n",
    "    \"    - for condition 'C' and 'R':\\n\"\n",
    "    \"       - baseline<baseline_number>\\n\"\n",
    "    \"           - baseline_number:\\n\"\n",
    "    \"               - '1' for baseline before tasks in condition\\n\"\n",
    "    \"               - '2' for baseline after tasks in condition\\n\"\n",
    "    \"- value_name (e.g., CORR): Value type\\n\"\n",
    "    \"    - 'CORR' for correlation value\\n\"\n",
    "    \"    - 'SHIFT' for shift difference\\n\"\n",
    "    \"- measure_type (e.g., HR): Type of measure\\n\"\n",
    "    \"    - 'HR' for Heart Rate\\n\"\n",
    "    \"    - 'SDNN' for Standard Deviation of NN-intervals\\n\"\n",
    "    \"    - 'RMSSD' for Root Mean Square of Successive Differences of NN-intervals\\n\"\n",
    ")\n",
    "\n",
    "# Add the note as a comment to cell A1\n",
    "comment = Comment(note, \"HS\")\n",
    "\n",
    "# Estimate width and height based on text length\n",
    "# Each character approximates about 5 pixels in width, and each line about 20 pixels in height\n",
    "approx_width = min(\n",
    "    10000, max(1000, len(note) * 5 // 100)\n",
    ")  # Width is capped between 300 and 800 points\n",
    "approx_height = min(\n",
    "    5000, max(1000, note.count(\"\\n\") * 20 + 100)\n",
    ")  # Height is capped between 300 and 600 points\n",
    "\n",
    "# Set comment dimensions\n",
    "comment.width = approx_width\n",
    "comment.height = approx_height\n",
    "\n",
    "ws[\"A1\"].comment = comment\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(\n",
    "    ANALYSIS_DATA_DIR\n",
    "    / \"ANOVA\"\n",
    "    / str(corr_name + \"_\" + result_choice + \"_anova_data.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anova merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_patterns = {\n",
    "    \"meas_number\": r\"^(1|2)\",\n",
    "    \"condition\": r\"_(C|R)_\",\n",
    "    \"task\": r\"_(baseline1|baseline2|z\\d(?:_\\d_[fm])?)_\",\n",
    "    \"value_name\": r\"_(CORR|SHIFT)_\",\n",
    "    \"measure_type\": r\"_(HR|SDNN|RMSSD)_\",\n",
    "    \"corr_type\": r\"_(pearson|spearman)$\",\n",
    "}\n",
    "\n",
    "task_order = [\n",
    "    \"baseline1\",\n",
    "    \"z\",\n",
    "    \"z1\",\n",
    "    \"z1_1_f\",\n",
    "    \"z1_2_m\",\n",
    "    \"z1_3_f\",\n",
    "    \"z1_4_m\",\n",
    "    \"z1_5_f\",\n",
    "    \"z1_6_m\",\n",
    "    \"z2\",\n",
    "    \"z2_1_m\",\n",
    "    \"z2_2_f\",\n",
    "    \"z2_3_m\",\n",
    "    \"z2_4_f\",\n",
    "    \"z2_5_m\",\n",
    "    \"z2_6_f\",\n",
    "    \"baseline2\",\n",
    "]\n",
    "\n",
    "sorting_rules = {\n",
    "    \"meas_number\": [\"1\", \"2\"],\n",
    "    \"condition\": [\"R\", \"C\"],\n",
    "    \"task\": task_order,\n",
    "    \"measure_type\": [\"HR\", \"SDNN\", \"RMSSD\"],\n",
    "    \"corr_type\": [\"pearson\", \"spearman\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written to c:\\Users\\huber\\OneDrive\\Dokumenty\\GitHub\\swps_synchronization_study\\results\\analysis_data\\ANOVA\\best_anova_data.xlsx\n",
      "DataFrame successfully written to c:\\Users\\huber\\OneDrive\\Dokumenty\\GitHub\\swps_synchronization_study\\results\\analysis_data\\ANOVA\\all_anova_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define the prefixes and suffixes for the results\n",
    "result_prefixes = [\"best\", \"all\"]\n",
    "corr_types = [\"pearson\", \"spearman\"]\n",
    "\n",
    "# Initialize empty lists to hold the merged DataFrames\n",
    "merged_results = []\n",
    "\n",
    "# Loop through each combination of prefix and correlation type for best results\n",
    "for prefix in result_prefixes:\n",
    "    for corr_type in corr_types:\n",
    "        # Construct the file path\n",
    "        file_path = (\n",
    "            RESULTS_DIR\n",
    "            / \"analysis_data\"\n",
    "            / \"ANOVA\"\n",
    "            / str(f\"{corr_type}_{prefix}_anova_data.xlsx\")\n",
    "        )\n",
    "\n",
    "        # Read the Excel file\n",
    "        try:\n",
    "            results_df = read_excel_file(file_path)\n",
    "            merged_results.append((corr_type, results_df))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Merge the results based on a common column, e.g., 'pair_number'\n",
    "    if len(merged_results) > 0:\n",
    "        # Start with the first DataFrame and add a suffix to all columns except 'pair_number'\n",
    "        first_corr_type = merged_results[0][0]\n",
    "        merged_df = merged_results[0][1].rename(\n",
    "            columns=lambda x: f\"{x}_{first_corr_type}\" if x != \"pair_number\" else x\n",
    "        )\n",
    "\n",
    "        for corr_type, df in merged_results[1:]:\n",
    "            # Ensure 'pair_number' exists in both DataFrames\n",
    "            if \"pair_number\" in merged_df.columns and \"pair_number\" in df.columns:\n",
    "                # Add suffix to the current DataFrame, except for 'pair_number'\n",
    "                df = df.rename(\n",
    "                    columns=lambda x: f\"{x}_{corr_type}\" if x != \"pair_number\" else x\n",
    "                )\n",
    "                merged_df = merged_df.merge(\n",
    "                    df, on=\"pair_number\", suffixes=(\"\", f\"_{corr_type}\")\n",
    "                )\n",
    "            else:\n",
    "                print(f\"'pair_number' not found in one of the DataFrames: {corr_type}\")\n",
    "\n",
    "        # Sort and reset index\n",
    "        merged_df = sort_columns_by_rules(merged_df, regex_patterns, sorting_rules)\n",
    "\n",
    "        # Save the merged results to an Excel file\n",
    "        write_to_excel(\n",
    "            merged_df,\n",
    "            RESULTS_DIR / \"analysis_data\" / \"ANOVA\" / str(prefix + \"_anova_data.xlsx\"),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Not enough data to merge {prefix} results.\")\n",
    "\n",
    "    # Clear merged_results for the next prefix\n",
    "    merged_results.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swps_synchronization_study_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
